{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import pickle\n",
    "import time\n",
    "from utils import _save, _load, SaveData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from model_build.dnn_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载词序列和embedding_matrix\n",
    "save_data = _load('./dataset/glove_embedding_data.pkl')\n",
    "train_pad_1 = save_data.train_pad_1\n",
    "train_pad_2 = save_data.train_pad_2\n",
    "labels = save_data.labels\n",
    "test_pad_1 = save_data.test_pad_1\n",
    "test_pad_2 = save_data.test_pad_2\n",
    "test_ids = save_data.test_ids\n",
    "embedding_matrix = save_data.embedding_matrix\n",
    "num_words = save_data.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载特征\n",
    "fs_basic = pd.read_csv('feature_store/train_feature_basic.csv')\n",
    "fs_fuzz = pd.read_csv('feature_store/train_feature_fuzz.csv')\n",
    "fs_w2v_gnews = pd.read_csv('feature_store/train_feature_w2v_gnews.csv')\n",
    "fs_tfidf = pd.read_csv('feature_store/train_feature_tfidf.csv')\n",
    "fs_w2v_glove = pd.read_csv('feature_store/train_feature_w2v_glove.csv')\n",
    "fs_graph = pd.read_csv('feature_store/train_feature_graph.csv')\n",
    "fs_freq = pd.read_csv('feature_store/train_feature_freq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充缺失值\n",
    "fs_w2v_gnews.fillna(0,inplace=True)\n",
    "fs_w2v_glove.fillna(0,inplace=True)\n",
    "fs_tfidf.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接特征\n",
    "X = np.hstack((fs_basic, fs_fuzz, fs_w2v_gnews, fs_tfidf, fs_w2v_glove, fs_freq, fs_graph))\n",
    "# 处理异常值\n",
    "X[np.isinf(X)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载测试数据\n",
    "fs_basic_test = pd.read_csv('feature_store/test_feature_basic.csv')\n",
    "fs_fuzz_test = pd.read_csv('feature_store/test_feature_fuzz.csv')\n",
    "fs_w2v_gnews_test = pd.read_csv('feature_store/test_feature_w2v_gnews.csv')\n",
    "fs_tfidf_test = pd.read_csv('feature_store/test_feature_tfidf.csv')\n",
    "fs_w2v_glove_test = pd.read_csv('feature_store/test_feature_w2v_glove.csv')\n",
    "fs_graph_test = pd.read_csv('feature_store/test_feature_graph.csv')\n",
    "fs_freq_test = pd.read_csv('feature_store/test_feature_freq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充缺失值\n",
    "fs_w2v_gnews_test.fillna(0,inplace=True)\n",
    "fs_w2v_glove_test.fillna(0,inplace=True)\n",
    "fs_tfidf_test.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接特征\n",
    "X_test = np.hstack((fs_basic_test, fs_fuzz_test, fs_w2v_gnews_test, fs_tfidf_test, fs_w2v_glove_test, fs_freq_test, fs_graph_test))\n",
    "# 处理异常值\n",
    "X_test[np.isinf(X_test)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练lgbm，保存模型\n",
    "def train_lgbm(X_train, y_train, X_valid, y_valid, fold_round, time_dict):\n",
    "    start = time.time()\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary', # 目标函数\n",
    "        'metric': 'binary_logloss', # 设置提升类型\n",
    "        'num_leaves': 47, # 叶子节点数\n",
    "        'learning_rate': 0.02, # 学习速率\n",
    "        'feature_fraction': 0.75, # 建树的特征选择比例\n",
    "        'bagging_fraction': 0.8, # 建树的样本采样比例\n",
    "        'bagging_freq': 5, # k 意味着每 k 次迭代执行bagging\n",
    "        'verbose': 0, # <0 显示致命的, =0 显示错误 (警告), >0 显示信息\n",
    "        'save_binary': True,\n",
    "        'min_data_in_leaf': 100, \n",
    "        'max_bin': 1023,\n",
    "    }\n",
    "    \n",
    "    lgbm_train = lgbm.Dataset(X_train, y_train)\n",
    "    lgbm_valid = lgbm.Dataset(X_valid, y_valid, reference=lgbm_train)\n",
    "\n",
    "    lgbm_bst = lgbm.train(params, lgbm_train, num_boost_round=4000, \n",
    "                 valid_sets=lgbm_valid, valid_names='valid',\n",
    "                 early_stopping_rounds=20, verbose_eval=False)\n",
    "    \n",
    "    y_pred = lgbm_bst.predict(X_valid, num_iteration=lgbm_bst.best_iteration)\n",
    "    df_pred = pd.DataFrame({'y_true':y_valid,'y_pred':y_pred})\n",
    "    df_pred.to_csv('./model_store/lgbm_model/lgbm_pred_{}.csv'.format(fold_round), index=False)\n",
    "    \n",
    "    y_pred_test = lgbm_bst.predict(X_test, num_iteration=lgbm_bst.best_iteration)\n",
    "    submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':y_pred_test})\n",
    "    submission.to_csv('./model_store/lgbm_model/lgbm_submission_{}.csv'.format(fold_round), index=False)\n",
    "    \n",
    "    lgbm_bst.save_model('./model_store/lgbm_model/lgbm_{}.model'.format(fold_round))\n",
    "    \n",
    "    time_dict['lgbm'].append(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练深度学习网络，保存模型，保存训练过程\n",
    "def train_DNN(seq_train_1, seq_train_2, X_train, y_train,\n",
    "              seq_valid_1, seq_valid_2, X_valid, y_valid,\n",
    "              fold_round, time_dict):\n",
    "    start = time.time()\n",
    "    sequence_length = seq_train_1.shape[1]\n",
    "    embedding_dim = embedding_matrix.shape[1]\n",
    "    \n",
    "    model = build_model_lstm_cnn_fs_v2(\n",
    "        feature_num=X.shape[1],\n",
    "        num_words=num_words,\n",
    "        embedding_dim=embedding_dim,\n",
    "        embedding_matrix=embedding_matrix,\n",
    "        max_sequence_length=sequence_length,\n",
    "        rate_drop_lstm=0.2,\n",
    "        rate_drop_dense=0.4\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    best_model_path = 'model_store/dnn_model/dnn_model_{}.h5'.format(fold_round)\n",
    "    model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "    \n",
    "    hist = model.fit(\n",
    "        [seq_train_1, seq_train_2, X_train],y_train, \n",
    "        validation_data=([seq_valid_1, seq_valid_2, X_valid], y_valid),\n",
    "        epochs=50,\n",
    "        batch_size=512,\n",
    "        shuffle=True, \n",
    "        class_weight='auto',\n",
    "        callbacks=[early_stopping, model_checkpoint],\n",
    "        verbose = 0)\n",
    "    \n",
    "    df_hist = pd.DataFrame(hist.history)\n",
    "    df_hist.to_csv('model_store/dnn_model/dnn_model_history_{}.csv'.format(fold_round), index=False)\n",
    "    \n",
    "    model.load_weights(best_model_path)\n",
    "    y_pred = model.predict([seq_valid_1, seq_valid_2, X_valid], batch_size=2048, verbose=0)\n",
    "    df_pred = pd.DataFrame({'y_true':y_valid,'y_pred':y_pred.ravel()})\n",
    "    df_pred.to_csv('./model_store/dnn_model/dnn_pred_{}.csv'.format(fold_round), index=False)\n",
    "    \n",
    "    y_pred_test = model.predict([test_pad_1, test_pad_2, X_test], batch_size=2048, verbose=0)\n",
    "    submission = pd.DataFrame({\"test_id\": test_ids, \"is_duplicate\": y_pred_test.ravel()})\n",
    "    submission.to_csv('./model_store/dnn_model/dnn_submission_{}.csv'.format(fold_round), index=False)\n",
    "    \n",
    "    time_dict['DNN'].append(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10折交叉验证\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0 start···························\n",
      "DNN train start·····\n",
      "Round 1 start···························\n",
      "DNN train start·····\n",
      "Round 2 start···························\n",
      "DNN train start·····\n",
      "Round 3 start···························\n",
      "DNN train start·····\n",
      "Round 4 start···························\n",
      "DNN train start·····\n",
      "Round 5 start···························\n",
      "DNN train start·····\n",
      "Round 6 start···························\n",
      "DNN train start·····\n",
      "Round 7 start···························\n",
      "DNN train start·····\n",
      "Round 8 start···························\n",
      "DNN train start·····\n",
      "Round 9 start···························\n",
      "DNN train start·····\n"
     ]
    }
   ],
   "source": [
    "time_dict = {\n",
    "    'lgbm':[]\n",
    "    'DNN':[]\n",
    "}\n",
    "y = np.array(labels)\n",
    "for fold_round, fold_indexs in enumerate(skf.split(X, y)):\n",
    "    print('Round {} start···························'.format(fold_round))\n",
    "    train_index, valid_index = fold_indexs\n",
    "    \n",
    "    seq_train_1 = train_pad_1[train_index]\n",
    "    seq_train_2 = train_pad_2[train_index]\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    \n",
    "    seq_valid_1 = train_pad_1[valid_index]\n",
    "    seq_valid_2 = train_pad_2[valid_index]\n",
    "    X_valid = X[valid_index]\n",
    "    y_valid = y[valid_index]\n",
    "    \n",
    "    # 训练lgbm\n",
    "    print('LightGBM train start·····')\n",
    "    train_lgbm(X_train, y_train, X_valid, y_valid, fold_round, time_dict)\n",
    "    # 训练DNN\n",
    "    print('DNN train start·····')\n",
    "    train_DNN(seq_train_1, seq_train_2, X_train, y_train,\n",
    "              seq_valid_1, seq_valid_2, X_valid, y_valid,\n",
    "              fold_round, time_dict)\n",
    "\n",
    "df_time = pd.DataFrame(time_dict)\n",
    "df_time.to_csv('model_store/10fold_time_dnn.csv', index=False)\n",
    "lgbm_submission = pd.DataFrame({'test_id':test_ids})\n",
    "dnn_submission = pd.DataFrame({'test_id':test_ids})\n",
    "lgbm_submission['is_duplicate'] = 0\n",
    "dnn_submission['is_duplicate'] = 0\n",
    "for i in range(10):\n",
    "    lgbm_part = pd.read_csv('./model_store/lgbm_model/lgbm_submission_{}.csv'.format(i))\n",
    "    dnn_part = pd.read_csv('./model_store/dnn_model/dnn_submission_{}.csv'.format(i))\n",
    "    lgbm_submission['is_duplicate'] = lgbm_submission.is_duplicate + lgbm_part.is_duplicate\n",
    "    dnn_submission['is_duplicate'] = dnn_submission.is_duplicate + dnn_part.is_duplicate\n",
    "\n",
    "lgbm_submission['is_duplicate'] /= 10\n",
    "dnn_submission['is_duplicate'] /= 10\n",
    "lgbm_submission.to_csv('submission/lgbm_10stack_submission.csv', index=False)\n",
    "dnn_submission.to_csv('submission/dnnV2_10stack_submission.csv', index=False)\n",
    "# 转换分布\n",
    "a = 0.174264424749 / 0.369197853026\n",
    "b = (1 - 0.174264424749) / (1 - 0.369197853026)\n",
    "lgbm_submission['is_duplicate'] = lgbm_submission.is_duplicate.apply(lambda x: a * x / (a * x + b * (1 - x)))\n",
    "dnn_submission['is_duplicate'] = dnn_submission.is_duplicate.apply(lambda x: a * x / (a * x + b * (1 - x)))\n",
    "lgbm_submission.to_csv('submission/lgbm_10stack_submission_trans.csv', index=False)\n",
    "dnn_submission.to_csv('submission/dnnV2_10stack_submission_trans.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ai]",
   "language": "python",
   "name": "conda-env-ai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
